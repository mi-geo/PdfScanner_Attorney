{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d30ed8b-9d52-43e3-8d3c-6d1ee50a0e6b",
   "metadata": {},
   "source": [
    "# Convert PDF to CSV, with pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ae6e8-e2ad-47aa-9e69-2a398b267fd4",
   "metadata": {},
   "source": [
    "### High light:\n",
    "1. for very organized PDF files, plumber might be a smart, fast way to convert files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5c7e8-bf66-4316-ac04-1facf32fe49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "directory = r\"C:\\Users\\46798566/Box\\Py_codes\\TexasCourt\\Book\\Attorney_list\"\n",
    "den_files = (\n",
    "            glob.glob(os.path.join(directory, \"AttorneyPopulationDensity*.pdf\")) +\n",
    "            glob.glob(os.path.join(directory, \"PopulationDensityReport*.pdf\"))\n",
    "            )\n",
    "#pdf_text = open(\"extracted_text.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6b72f-3d5a-461b-aaf8-2e516ca3436d",
   "metadata": {},
   "source": [
    "### do one file first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb64817-c6d6-422a-babb-b0d8d1fc7c07",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up what kinda table will be extracted:\n",
    "table_settings = {\n",
    "    \"vertical_strategy\": \"text\",\n",
    "    \"horizontal_strategy\": \"text\",\n",
    "    # \"intersection_tolerance\": 5,\n",
    "    # \"snap_tolerance\": 3,\n",
    "    # \"join_tolerance\": 6,\n",
    "    # \"edge_min_length\": 3,\n",
    "}\n",
    "\n",
    "# to detect the county names only...\n",
    "table_settings2 = {\n",
    "    \"vertical_strategy\": \"lines\",\n",
    "    \"horizontal_strategy\": \"text\",\n",
    "    # \"intersection_tolerance\": 5,\n",
    "    # \"snap_tolerance\": 3,\n",
    "    # \"join_tolerance\": 6,\n",
    "    # \"edge_min_length\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db3e10-a8a3-444c-8d7b-83d1d9ac3455",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_rows_per_page(df):   # find out which col has county names\n",
    "    county_counts = (\n",
    "        df.astype(str)\n",
    "          .apply(lambda col: col.str.contains(r\"\\bCount\", case=True, na=False).sum())\n",
    "          .sort_values(ascending=False)\n",
    "    )\n",
    "    county_col = county_counts.idxmax()\n",
    "    df_reduced = df[\n",
    "        df[county_col]\n",
    "          .astype(str)\n",
    "          .str.contains(r\"\\bCount\", case=True, na=False)  # 'count, county, etc\n",
    "    ]\n",
    "    #print('there are' , len(df_reduced), \"rows in the returned table. There were\", len(df), 'rows...')\n",
    "    if len(df_reduced) > 0 :\n",
    "        print('Counties include:', df_reduced[county_col].iloc[0], 'and other', len(df_reduced)-1, 'counties')\n",
    "    else:\n",
    "        print('No counties in this \"table\". There were', len(df)-1, 'lines before.')\n",
    "    return(df_reduced)\n",
    "\n",
    "#table = tables[0]    #\n",
    "\n",
    "target_file = den_files[3]\n",
    "pdf =  pdfplumber.open(target_file)\n",
    "\n",
    "len(all_tables_on_pdf)\n",
    "table_df = pd.DataFrame(all_tables_on_pdf[10]) \n",
    "\n",
    "# extract all the tables....\n",
    "all_tables_on_pdf = []\n",
    "all_tables_on_pdf_para = []\n",
    "for page in pdf.pages:\n",
    "    tables = page.extract_tables(table_settings=table_settings) # no result, return [] \n",
    "    tables_para = page.extract_tables(table_settings=table_settings2) # no result, return []   # the two strategies should have the same number of rows.. table para contains the name of counties. \n",
    "    print(\"Now processing page \", page.page_number, \".\", sep = \"\")\n",
    "    all_tables_on_pdf.extend(tables)\n",
    "    all_tables_on_pdf_para.extend(tables_para)\n",
    "\n",
    "# then we have a huge list of tables...\n",
    "all_tables_on_pdf_reduced = []\n",
    "num_tables = len(all_tables_on_pdf)\n",
    "for x in range(num_tables):\n",
    "    table_df = pd.DataFrame(all_tables_on_pdf[x])\n",
    "    #table_para_df = pd.DataFrame(all_tables_on_pdf_para[x])\n",
    "\n",
    "    print(len(table_df), len(table_para_df))\n",
    "    #if (len(table_df) == len(table_para_df)):\n",
    "    #    table_df = pd.concat([table_df, table_para_df], axis=1)\n",
    "    #else:\n",
    "    #    pass\n",
    "    table_df_reduced = select_rows_per_page(table_df)\n",
    "    all_tables_on_pdf_reduced.append(table_df_reduced)\n",
    "all_tables_as_df = pd.concat(all_tables_on_pdf_reduced, ignore_index = True)\n",
    "clean_df = clean_table(all_tables_as_df)\n",
    "\n",
    "clean_df.to_csv(\"table_test.csv\")\n",
    "\n",
    "\n",
    "pd.all_tables_as_df\n",
    "print(all_tables_as_df)\n",
    "df = pd.DataFrame(table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[3].str.contains(r'county', case = False)\n",
    "\n",
    "df[1]\n",
    "print(table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with pdfplumber.open(target_file) as pdf:\n",
    "    for p in range(start_page - 1, end_page):\n",
    "        page = pdf.pages[p]\n",
    "        tables = page.extract_tables(table_settings=table_settings) or []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a183648-093f-49e5-b524-4f6b1912549f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### use ChatGPT:\n",
    "i want for each row, when there were empty col, move the rest col to the left. then combine the col where there is name 'County' with the previous one, if the first letter is not . or Capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73526142-2053-4935-bf75-3c52249f9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### the following table is entirely defined by Chatgpt. Let me see if it works:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def _is_empty(x):\n",
    "    if x is None:\n",
    "        return True\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return True\n",
    "    return str(x).strip() == \"\"\n",
    "\n",
    "def _shift_left_row(row_vals):\n",
    "    vals = [v for v in row_vals if not _is_empty(v)]\n",
    "    return vals + [None] * (len(row_vals) - len(vals))\n",
    "\n",
    "def _needs_merge_county(cell):\n",
    "    s = str(cell).strip()\n",
    "    if \"County\" not in s:\n",
    "        return False\n",
    "    if not s:\n",
    "        return False\n",
    "    first = s[0]\n",
    "    # merge when first letter is NOT '.' and NOT uppercase\n",
    "    return (first != \".\") and (not first.isupper())\n",
    "\n",
    "def clean_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # 1) shift left per row\n",
    "    out = out.apply(lambda r: pd.Series(_shift_left_row(r.tolist()), index=out.columns), axis=1)\n",
    "\n",
    "    # 2) merge \"* County\" fragments into previous cell (when fragment starts lowercase etc.)\n",
    "    def merge_row(row):\n",
    "        vals = row.tolist()\n",
    "        for i in range(1, len(vals)):\n",
    "            if _is_empty(vals[i]) or _is_empty(vals[i-1]):\n",
    "                continue\n",
    "            if _needs_merge_county(vals[i]):\n",
    "                prev = str(vals[i-1]).rstrip()\n",
    "                cur  = str(vals[i]).lstrip()\n",
    "                # join without adding a space (works for \".Har\" + \"din County\")\n",
    "                vals[i-1] = re.sub(r\"\\s+\", \" \", (prev + cur)).strip()\n",
    "                vals[i] = None\n",
    "        # shift left again after merges\n",
    "        return pd.Series(_shift_left_row(vals), index=row.index)\n",
    "\n",
    "    out = out.apply(merge_row, axis=1)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83cd8b-bb37-483c-8150-1b79595e49bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### a more comprehensive version: \n",
    "# Row-wise left shift (remove empty cells by shifting remaining values left).\n",
    "# Merge “County” fragments into the previous cell when the “County” cell starts with neither . nor uppercase (e.g., \"an County\").\n",
    "# Merge stray dot cells: if a cell is exactly \".\" and the cell to its right starts with a capital letter, merge them (so \".\" + \"Callah\" → \".Callah\"), then shift left again.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def _is_empty(x):\n",
    "    if x is None:\n",
    "        return True\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return True\n",
    "    return str(x).strip() == \"\"\n",
    "\n",
    "def _shift_left(vals, width):\n",
    "    keep = [v for v in vals if not _is_empty(v)]\n",
    "    return keep + [None] * (width - len(keep))\n",
    "\n",
    "def _starts_with_capital(x):\n",
    "    s = str(x).strip()\n",
    "    return len(s) > 0 and s[0].isupper()\n",
    "\n",
    "def clean_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    ncol = out.shape[1]\n",
    "\n",
    "    def clean_one_row(row):\n",
    "        vals = row.tolist()\n",
    "\n",
    "        # (1) shift left to remove empties\n",
    "        vals = _shift_left(vals, ncol)\n",
    "\n",
    "        # (2) merge \"County\" fragments into previous cell if first char is not '.' and not uppercase\n",
    "        for i in range(1, ncol):\n",
    "            if _is_empty(vals[i]) or _is_empty(vals[i-1]):\n",
    "                continue\n",
    "            s = str(vals[i]).strip()\n",
    "            if \"County\" in s:\n",
    "                first = s[0] if s else \"\"\n",
    "                if (first != \".\") and (not first.isupper()):\n",
    "                    prev = str(vals[i-1]).rstrip()\n",
    "                    cur  = str(vals[i]).lstrip()\n",
    "                    vals[i-1] = re.sub(r\"\\s+\", \" \", (prev + cur)).strip()\n",
    "                    vals[i] = None\n",
    "\n",
    "        vals = _shift_left(vals, ncol)\n",
    "\n",
    "        # (3) merge stray '.' with right cell if right starts with capital\n",
    "        for i in range(0, ncol - 1):\n",
    "            if str(vals[i]).strip() == \".\" and not _is_empty(vals[i+1]) and _starts_with_capital(vals[i+1]):\n",
    "                vals[i] = \".\" + str(vals[i+1]).lstrip()\n",
    "                vals[i+1] = None\n",
    "\n",
    "        vals = _shift_left(vals, ncol)\n",
    "        return pd.Series(vals, index=row.index)\n",
    "\n",
    "    out = out.apply(clean_one_row, axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c310600a-5269-4573-8357-7a4ee3917888",
   "metadata": {},
   "source": [
    "## next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b5921b-02dc-4540-ad85-3a9a252bff61",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "defaultdict(<function <lambda> at 0x000002394E92ECA0>, {'x': 999})\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "pdf_path = r\"C:\\Users\\46798566\\Downloads\\AttorneyPopulationDensity2014-15.pdf\"\n",
    "out_csv  = r\"C:\\Users\\46798566\\Downloads\\AttorneyPopulationDensity2014-15_pages14-20_combined.csv\"\n",
    "out_notable = r\"C:\\Users\\46798566\\Downloads\\AttorneyPopulationDensity2014-15_pages14-20_no_table_pages.txt\"\n",
    "\n",
    "start_page = 14\n",
    "end_page   = 20  # inclusive\n",
    "\n",
    "table_settings = {\n",
    "    \"vertical_strategy\": \"lines\",\n",
    "    \"horizontal_strategy\": \"lines\",\n",
    "    \"intersection_tolerance\": 5,\n",
    "    \"snap_tolerance\": 3,\n",
    "    \"join_tolerance\": 3,\n",
    "    \"edge_min_length\": 3,\n",
    "}\n",
    "\n",
    "def clean_df_from_table(table):\n",
    "    df = pd.DataFrame(table)\n",
    "    df = df.dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # Use first row as header if it looks header-ish\n",
    "    if len(df) >= 2:\n",
    "        header = df.iloc[0].tolist()\n",
    "        if sum(bool(h) for h in header) >= sum(bool(x) for x in df.iloc[1].tolist()):\n",
    "            df.columns = header\n",
    "            df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "all_parts = []\n",
    "no_table_pages = []\n",
    "canonical_cols = None\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for p in range(start_page - 1, end_page):\n",
    "        page = pdf.pages[p]\n",
    "        tables = page.extract_tables(table_settings=table_settings) or []\n",
    "\n",
    "        if len(tables) == 0:\n",
    "            no_table_pages.append(p + 1)  # human page number\n",
    "            continue\n",
    "\n",
    "        for t in tables:\n",
    "            df = clean_df_from_table(t)\n",
    "            if df.empty:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d1af509-0351-41bd-8d68-c94d283316f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\ArcGIS\\\\Pro\\\\bin\\\\Python\\\\envs\\\\arcgispro-py3\\\\python.exe'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea47100-142d-454d-a505-c36aa13db6a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8012e730-b56b-4612-8ccd-759af6f63fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the table, and convert it too a united format (first make sure no missing values...)\n",
    "# combine each tab to a big data frame, and then convert it to a wide format , make sure the names on the county stay the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ecdeb-f759-408a-85e4-df1afe8b851c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a38ed-eddb-43d4-81cf-47279c3f795b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_path = r\"C:\\Users\\46798566\\Downloads\\Texas_counties.xlsx\"   # <-- change this\n",
    "\n",
    "# read all sheets\n",
    "sheets = pd.read_excel(excel_path, sheet_name=None)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for year, df in sheets.items():\n",
    "    y = df['year'].iloc[0]\n",
    "    \n",
    "    df = df.copy()\n",
    "    dfs.append(df)\n",
    "\n",
    "big_long = pd.concat(dfs, ignore_index=True)\n",
    "big_long = big_long.iloc[:, :7]\n",
    "big_long['year']  = pd.to_numeric(big_long.iloc[:, 0]).astype(\"Int64\")\n",
    "\n",
    "\n",
    "county_col = \"county name\"\n",
    "value_cols = [c for c in big_long.columns if c not in [county_col, \"year\"]]\n",
    "\n",
    "wide = big_long.pivot(\n",
    "    index= county_col,\n",
    "    columns= \"year\",\n",
    "    values=value_cols\n",
    "    #aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# flatten column index\n",
    "wide.columns = [f\"{col}__{year}\" for col, year in wide.columns]\n",
    "wide = wide.reset_index()\n",
    "big_long.to_csv(\"wide.csv\")\n",
    "wide.to_csv(\"wide.csv\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd3c98-c222-4399-96cb-09eafb30af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume county column is literally named \"county name\"\n",
    "county_col = \"county name\"\n",
    "year_col = \"year\"\n",
    "big_long[year_col] = pd.to_numeric(big_long[year_col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "value_cols = [c for c in big_long.columns if c not in [county_col, \"year\"]]\n",
    "\n",
    "\n",
    "wide = big_long.pivot_table(\n",
    "    index=county_col,\n",
    "    columns=year_col,\n",
    "    values=value_cols,\n",
    "    aggfunc=\"first\" \n",
    ")\n",
    "\n",
    "# flatten column index\n",
    "wide.columns = [f\"{col}__{year}\" for col, year in wide.columns]\n",
    "wide = wide.reset_index()\n",
    "\n",
    "wide"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyCharm Env)",
   "language": "python",
   "name": "pycharm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
